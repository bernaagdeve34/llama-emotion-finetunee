{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 73458,
     "status": "aborted",
     "timestamp": 1750669071810,
     "user": {
      "displayName": "Berna Ağdeve",
      "userId": "01895766378471032317"
     },
     "user_tz": -180
    },
    "id": "0BUd1h_ftZ3z"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "baedb17b3f6545a69b27e78a93ca3bee",
      "4abb7d9900ef4583ab828197b858005a",
      "79b8d71a1db341639f1f676afa1d2bd1",
      "fda5669b8a194eeda167d2b28029dbac",
      "81b57c994f4849d2ad5c38fd18005a58",
      "cad8c18e18364add93aaaee71645ed93",
      "aa121907823349e6a3e548fe03ebb913",
      "1acf6b0d85b642a8970c48cacd975784",
      "eef50c4b97194356b2c2f62c945cbf32",
      "edf0f4b4c2cd4e709d74035c8896ccc1",
      "d71648a90cbc4ccd8c0d6ab3941ae70c"
     ]
    },
    "executionInfo": {
     "elapsed": 50842,
     "status": "ok",
     "timestamp": 1747946030956,
     "user": {
      "displayName": "Berna Ağdeve",
      "userId": "01895766378471032317"
     },
     "user_tz": -180
    },
    "id": "XY8EYYcoGn34",
    "outputId": "83228216-9d49-4c60-c416-b94361cabbd2"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install required libraries\n",
    "!pip install -q accelerate -U\n",
    "!pip install -q bitsandbytes -U\n",
    "!pip install -q trl -U\n",
    "!pip install -q peft -U\n",
    "!pip install -q transformers -U\n",
    "!pip install -q fsspec==2023.12.0\n",
    "!pip install -q gcsfs==2023.12.0\n",
    "\n",
    "# Load and split the dataset\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "raw_dataset = load_dataset(\"dair-ai/emotion\", split=\"train\")\n",
    "raw_dataset = raw_dataset.shuffle(seed=42).select(range(10000))\n",
    "\n",
    "train_testvalid = raw_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'validation': test_valid['train'],\n",
    "    'test': test_valid['test']\n",
    "})\n",
    "\n",
    "# Create prompts\n",
    "from transformers import AutoTokenizer\n",
    "template_tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "label_map = {\n",
    "    0: \"sadness\",\n",
    "    1: \"joy\",\n",
    "    2: \"love\",\n",
    "    3: \"anger\",\n",
    "    4: \"fear\",\n",
    "    5: \"surprise\"\n",
    "}\n",
    "\n",
    "def format_prompt(example):\n",
    "    prompt = f\"\"\"<|system|>\n",
    "Sen bir duygu analizi uzmanısın. Verilen metindeki duyguyu belirle. Sadece şu seçeneklerden birini kullan: 'sadness', 'joy', 'love', 'anger', 'fear', 'surprise'.\n",
    "</s>\n",
    "<|user|>\n",
    "Metin: {example['text']}\n",
    "Bu metindeki duygu nedir? Sadece duygu etiketini yaz.\n",
    "</s>\n",
    "<|assistant|>\n",
    "{label_map[example['label']]}\n",
    "</s>\"\"\"\n",
    "    return {\"text\": prompt, \"label\": example[\"label\"]}\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].map(format_prompt)\n",
    "\n",
    "# Print example\n",
    "print(dataset['train'][0])\n",
    "\n",
    "# Model inference\n",
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "pipe = pipeline(\"text-generation\", model=model_name, device='cuda')\n",
    "\n",
    "# QLoRA setup\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "!pip uninstall -y bitsandbytes\n",
    "!pip install -U bitsandbytes\n",
    "!pip install accelerate\n",
    "\n",
    "# QLoRA training preparation\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype='float16',\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM',\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj']  # Llama mimarisi için\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Tokenization\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1330277,
     "status": "ok",
     "timestamp": 1747947421659,
     "user": {
      "displayName": "Berna Ağdeve",
      "userId": "01895766378471032317"
     },
     "user_tz": -180
    },
    "id": "2klMWJjgHIEx",
    "outputId": "52220bd5-710c-4795-cb9b-d882a1aad1dc"
   },
   "outputs": [],
   "source": [
    "# Model training\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "\n",
    "output_dir = \"train_dir\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    args=args,\n",
    "    peft_config=peft_config\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 265123,
     "status": "ok",
     "timestamp": 1747948856112,
     "user": {
      "displayName": "Berna Ağdeve",
      "userId": "01895766378471032317"
     },
     "user_tz": -180
    },
    "id": "WjZkAeJkHm0w",
    "outputId": "72641ac4-64d7-49a9-92a1-4d0e17c6a9e2"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_label_from_output(output_text):\n",
    "    output_text = output_text.lower().strip()\n",
    "    output_text = output_text.replace('\"', '').strip()\n",
    "    words = output_text.split()\n",
    "    if not words:\n",
    "        return -1\n",
    "    first_word = words[0]\n",
    "    for label_id, label_str in label_map.items():\n",
    "        if first_word == label_str:\n",
    "            return label_id\n",
    "    for label_id, label_str in label_map.items():\n",
    "        if label_str in output_text:\n",
    "            return label_id\n",
    "    return -1\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "total_examples = len(dataset[\"test\"])\n",
    "\n",
    "for i, example in enumerate(tqdm(dataset[\"test\"])):\n",
    "    prompt = example[\"text\"].split(\"<|assistant|>\")[0] + \"<|assistant|>\"\n",
    "\n",
    "    try:\n",
    "        generated = pipe(\n",
    "            prompt,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2,\n",
    "            no_repeat_ngram_size=3,\n",
    "            temperature=0.1\n",
    "        )[0][\"generated_text\"]\n",
    "\n",
    "        response_text = generated[len(prompt):].strip()\n",
    "        pred = extract_label_from_output(response_text)\n",
    "\n",
    "        if pred != -1:\n",
    "            true_labels.append(example[\"label\"])\n",
    "            pred_labels.append(pred)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "if true_labels:\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    f1 = f1_score(true_labels, pred_labels, average=\"weighted\")\n",
    "    print(f\"\\nDoğruluk (Accuracy): {accuracy:.4f}\")\n",
    "    print(f\"F1 Skoru: {f1:.4f}\")\n",
    "else:\n",
    "    print(\"Geçerli tahmin yapılamadı.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gv0bcLFu4CBj"
   },
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(\"/content/drive/MyDrive/tinyllama-qlora-emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9c14M334LzS",
    "outputId": "58979318-a31e-451e-ae82-479fe3c81845"
   },
   "outputs": [],
   "source": [
    "# Interactive testing system\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\"/content/drive/MyDrive/tinyllama-qlora-emotion\", device_map=\"auto\")\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=merged_model, tokenizer=tokenizer)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Bir metin yazın (çıkmak için 'exit' yazın): \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Çıkılıyor...\")\n",
    "        break\n",
    "    prompt = f\"\"\"<|system|>\n",
    "Sen bir duygu analizi uzmanısın. Verilen metindeki duyguyu belirle. Sadece şu seçeneklerden birini kullan: 'sadness', 'joy', 'love', 'anger', 'fear', 'surprise'.\n",
    "</s>\n",
    "<|user|>\n",
    "Metin: {user_input}\n",
    "Bu metindeki duygu nedir? Sadece duygu etiketini yaz.\n",
    "</s>\n",
    "<|assistant|>\"\"\"\n",
    "    output = pipe(prompt, max_new_tokens=5, do_sample=False, return_full_text=False)[0]['generated_text']\n",
    "    model_response = output.strip()\n",
    "    print(f\"Tahmin edilen duygu: {model_response}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOnE+PBxSg+qG6g4GnGCpqf",
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
